from __future__ import annotations

from typing import Annotated, Generator, TypeAlias, TYPE_CHECKING, Iterable
from datasets import Dataset
import torch


def similarity_batcher() -> None:
    raise NotImplementedError


def tokenize() -> None:
    ...

def tokenize_dataset(dataset: Dataset):
    dataset.map(tokenize)


TSeq: TypeAlias = Annotated[torch.Tensor, "B", "T", "D"]
TMemKeys: TypeAlias = Annotated[torch.Tensor, "M", "K"]
TMemVals: TypeAlias = Annotated[torch.Tensor, "M", "V"]


def sequencer(
    dataset: Iterable[str], seq_len: int
) -> Generator[tuple[str, str], None, None]:
    for sample in dataset:
        for i in range(len(sample) // seq_len):  # TODO +1 here?
            sub_sample = sample[i * seq_len : (i + 1) * seq_len + 1]  # noqa
            yield sub_sample[:-1], sub_sample[1:]


class Batches:
    def __iter__(self) -> Batches:
        raise NotImplementedError

    def __next__(self) -> tuple[TSeq, TSeq, TMemKeys, TMemVals]:
        raise NotImplementedError
